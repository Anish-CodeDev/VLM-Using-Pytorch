{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1ab2ee25-c78f-4079-81ea-b01e32735ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertModel,pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_16,ViT_B_16_Weights\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe,build_vocab_from_iterator\n",
    "import math\n",
    "from PIL import Image\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9253542a-aa57-40ec-bba1-eefcfe853e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d48565-9cec-4a0b-8642-164be2b9fc56",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anish\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a523159f-7325-4914-888a-f14e817ae982",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7a369f2-3be3-4cf0-b143-74021db2d43a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anish\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "gen = pipeline(model='openai-community/gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "78174efd-ad25-4ddf-ada9-2e1cb65c6c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'image',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'a',\n",
       " 'white',\n",
       " 'object',\n",
       " 'which',\n",
       " 'may',\n",
       " 'be',\n",
       " '\\xa0a',\n",
       " 'small,',\n",
       " 'white',\n",
       " 'object,',\n",
       " 'or',\n",
       " 'a',\n",
       " 'large,',\n",
       " 'white',\n",
       " 'object.',\n",
       " 'The']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"The image consists of a white object which may be \",do_sample=False)[0]['generated_text'].split(' ')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3964879-92d5-4cda-a0bd-a8cf43f36c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35407405-7e2e-4af7-a8d9-08bcc8d210a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7a6e48a-e5b9-4f5d-8867-ceeebd12c462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:27<00:00, 14779.33it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_embedding = GloVe(name='6B',dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d42e835-3642-4ff5-a0bf-6d6f6824453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = []\n",
    "with open('data/captions.txt','r') as f:\n",
    "    for s in f.read().split('\\n'):\n",
    "        w = ''\n",
    "\n",
    "        for c in s.split()[1:]:\n",
    "            w  += c + \" \"\n",
    "\n",
    "        caption.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34ba1356-f9f2-44c6-af60-526a6a2965ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A man in street racer armor be examine the tire of another racer 's motorbike . \""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ba54495-fd8a-47f5-9e71-dd97c819a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(cap):\n",
    "    for c in cap:\n",
    "        yield tokenizer(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bef65bd-0cea-4533-a02e-c6ca3f383090",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(caption),specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "650898d9-d8bb-4e95-b234-ace932dc0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,seq_len):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        pe = torch.zeros(seq_len,d_model)\n",
    "\n",
    "        positions = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0,d_model,2).float() * (-math.log(10000)/d_model)\n",
    "        )\n",
    "        pe[:,0::2] = torch.sin(positions * div_term)\n",
    "        pe[:,1::2] = torch.cos(positions * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.pe\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e991ef2-a3d1-455e-8037-db70193ded9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self,rank,in_features,out_features,alpha):\n",
    "        super().__init__()\n",
    "        standard_deviation = 1/torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_features,rank) * standard_deviation)\n",
    "        self.B = nn.Parameter(torch.zeros(rank,out_features))\n",
    "        self.alpha = alpha\n",
    "        self.rank = rank\n",
    "\n",
    "    def forward(self,x):\n",
    "        return (self.alpha/self.rank) * (x@self.A@self.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3dd2afaa-137e-4d2a-abf0-144e543e1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRADenseLayer(nn.Module):\n",
    "    def __init__(self,linear,rank,alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear.to(device)\n",
    "        in_features = self.linear.in_features\n",
    "        out_features = self.linear.out_features\n",
    "        self.lora = LoRA(rank,in_features,out_features,alpha)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "acb1319a-531e-4456-89e0-199a0c52b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_encoder = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "abb1445f-e0eb-439a-afba-47d87e599f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(12):\n",
    "    vit_encoder.encoder.layers[n].mlp[0] = LoRA_Dense(vit_encoder.encoder.layers[n].mlp[0],2,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6e790095-86f2-4a83-93fd-6e5f068c50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self,n_head,num_encoder_layers,num_decoder_layers,max_seq_len):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = vit_encoder\n",
    "        self.vision_encoder.head = nn.Identity() # Does'nt modify anything\n",
    "\n",
    "        self.enc_projection = nn.Linear(1000,50)\n",
    "        self.token_embedding  = nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=True)\n",
    "        self.d_model = 50\n",
    "        self.positional_encoding = PositionalEncoding(self.d_model,max_seq_len)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=self.d_model,nhead=n_head,batch_first=True)\n",
    "        self.decoder_transformer = nn.TransformerDecoder(decoder_layer,num_layers=num_decoder_layers)\n",
    "        self.output_linear = nn.Linear(self.d_model,self.d_model)\n",
    "\n",
    "    def forward(self,img,cap):\n",
    "        enc_out = self.vision_encoder(img)\n",
    "        caption_embeddings = self.token_embedding(cap) * math.sqrt(self.d_model) + self.positional_encoding(cap)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(50,device=device)\n",
    "\n",
    "        decoder_output = self.decoder_transformer(\n",
    "            tgt=caption_embeddings,\n",
    "            memory=self.enc_projection(enc_out).unsqueeze(0),\n",
    "            tgt_mask=mask.unsqueeze(0).expand(50,-1,-1)\n",
    "        )\n",
    "        output = self.output_linear(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b161850b-0195-48f8-838e-2380e92329ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioningModel(50,6,6,50).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "68d8b10c-6635-4923-a2ca-a0051544046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict = torch.load('model/model-adam.pt',map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dd1178de-5865-4d6f-b79a-25f0b47c6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_obj = torchvision.transforms.Resize((216,216))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "87678bd1-f472-44e4-b667-1cc1990f96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    img = Image.open(f\"{img_path}\").convert('RGB')\n",
    "    img = resize_obj(img)\n",
    "    inp =  preprocess(img).unsqueeze(0).to(device)\n",
    "    cap = torch.tensor(vocab(tokenizer(\"<start>\")),dtype=torch.int64).unsqueeze(0).to(device)\n",
    "    print(inp.shape)\n",
    "    print(cap.shape)\n",
    "    out = model(inp,cap)\n",
    "    predicted = torch.argmax(out,dim=-1)\n",
    "    #predicted = torch.max(out.data,1)[1]\n",
    "    text = [vocab.lookup_token(idx) for idx in predicted[0]]\n",
    "    text = set(text)\n",
    "    return text\n",
    "    #return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2e775dc3-4987-4f8e-8d5a-e204e064a9f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "text = list(predict('image.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c353f240-8e9a-4381-9c0a-ebb855548742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'over']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9e20a21d-83ef-4db2-8f76-47edd3452d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4802ed4d-f7ad-49b1-88f3-17ab8bf12620",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    if i == len(text) - 1:\n",
    "        \n",
    "        w += text[i]\n",
    "    else:\n",
    "        w += text[i] + ' and '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "882eec3f-0d5b-490c-9e11-1e1ffed7260d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blue and over'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6bbce9d2-9322-45bf-a9f6-0feaa9ab90c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'image',\n",
       " 'has',\n",
       " 'characteristics',\n",
       " 'of',\n",
       " 'blue',\n",
       " 'and',\n",
       " 'overcould',\n",
       " 'you',\n",
       " 'tell',\n",
       " 'something',\n",
       " 'about',\n",
       " 'it?\\n\\nThe',\n",
       " 'image',\n",
       " 'is',\n",
       " 'a',\n",
       " 'composite',\n",
       " 'of',\n",
       " 'two',\n",
       " 'images.']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"This image has characteristics of \" + w +\"could you tell something about it\",do_sample=False)[0]['generated_text'].split(' ')[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
