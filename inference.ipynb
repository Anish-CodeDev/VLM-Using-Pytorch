{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1ab2ee25-c78f-4079-81ea-b01e32735ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertModel,pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_16,ViT_B_16_Weights\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe,build_vocab_from_iterator\n",
    "import math\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9253542a-aa57-40ec-bba1-eefcfe853e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d48565-9cec-4a0b-8642-164be2b9fc56",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anish\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a523159f-7325-4914-888a-f14e817ae982",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7a369f2-3be3-4cf0-b143-74021db2d43a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anish\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "gen = pipeline(model='openai-community/gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "78174efd-ad25-4ddf-ada9-2e1cb65c6c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'image',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'a',\n",
       " 'white',\n",
       " 'object',\n",
       " 'which',\n",
       " 'may',\n",
       " 'be',\n",
       " '\\xa0a',\n",
       " 'small,',\n",
       " 'white',\n",
       " 'object,',\n",
       " 'or',\n",
       " 'a',\n",
       " 'large,',\n",
       " 'white',\n",
       " 'object.',\n",
       " 'The']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"The image consists of a white object which may be \",do_sample=False)[0]['generated_text'].split(' ')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3964879-92d5-4cda-a0bd-a8cf43f36c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35407405-7e2e-4af7-a8d9-08bcc8d210a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7a6e48a-e5b9-4f5d-8867-ceeebd12c462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 399999/400000 [00:27<00:00, 14779.33it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_embedding = GloVe(name='6B',dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d42e835-3642-4ff5-a0bf-6d6f6824453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = []\n",
    "with open('data/captions.txt','r') as f:\n",
    "    for s in f.read().split('\\n'):\n",
    "        w = ''\n",
    "\n",
    "        for c in s.split()[1:]:\n",
    "            w  += c + \" \"\n",
    "\n",
    "        caption.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34ba1356-f9f2-44c6-af60-526a6a2965ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A man in street racer armor be examine the tire of another racer 's motorbike . \""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ba54495-fd8a-47f5-9e71-dd97c819a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(cap):\n",
    "    for c in cap:\n",
    "        yield tokenizer(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bef65bd-0cea-4533-a02e-c6ca3f383090",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(caption),specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "650898d9-d8bb-4e95-b234-ace932dc0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,seq_len):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        pe = torch.zeros(seq_len,d_model)\n",
    "\n",
    "        positions = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0,d_model,2).float() * (-math.log(10000)/d_model)\n",
    "        )\n",
    "        pe[:,0::2] = torch.sin(positions * div_term)\n",
    "        pe[:,1::2] = torch.cos(positions * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\",pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.pe\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e991ef2-a3d1-455e-8037-db70193ded9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self,rank,in_features,out_features,alpha):\n",
    "        super().__init__()\n",
    "        standard_deviation = 1/torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_features,rank) * standard_deviation)\n",
    "        self.B = nn.Parameter(torch.zeros(rank,out_features))\n",
    "        self.alpha = alpha\n",
    "        self.rank = rank\n",
    "\n",
    "    def forward(self,x):\n",
    "        return (self.alpha/self.rank) * (x@self.A@self.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3dd2afaa-137e-4d2a-abf0-144e543e1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRADenseLayer(nn.Module):\n",
    "    def __init__(self,linear,rank,alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear.to(device)\n",
    "        in_features = self.linear.in_features\n",
    "        out_features = self.linear.out_features\n",
    "        self.lora = LoRA(rank,in_features,out_features,alpha)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "acb1319a-531e-4456-89e0-199a0c52b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_encoder = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "abb1445f-e0eb-439a-afba-47d87e599f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(12):\n",
    "    vit_encoder.encoder.layers[n].mlp[0] = LoRA_Dense(vit_encoder.encoder.layers[n].mlp[0],2,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6e790095-86f2-4a83-93fd-6e5f068c50ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self,n_head,num_encoder_layers,num_decoder_layers,max_seq_len):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = vit_encoder\n",
    "        self.vision_encoder.head = nn.Identity() # Does'nt modify anything\n",
    "\n",
    "        self.enc_projection = nn.Linear(1000,50)\n",
    "        self.token_embedding  = nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=True)\n",
    "        self.d_model = 50\n",
    "        self.positional_encoding = PositionalEncoding(self.d_model,max_seq_len)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=self.d_model,nhead=n_head,batch_first=True)\n",
    "        self.decoder_transformer = nn.TransformerDecoder(decoder_layer,num_layers=num_decoder_layers)\n",
    "        self.output_linear = nn.Linear(self.d_model,self.d_model)\n",
    "\n",
    "    def forward(self,img,cap):\n",
    "        enc_out = self.vision_encoder(img)\n",
    "        caption_embeddings = self.token_embedding(cap) * math.sqrt(self.d_model) + self.positional_encoding(cap)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(50,device=device)\n",
    "\n",
    "        decoder_output = self.decoder_transformer(\n",
    "            tgt=caption_embeddings,\n",
    "            memory=self.enc_projection(enc_out).unsqueeze(0),\n",
    "            tgt_mask=mask.unsqueeze(0).expand(50,-1,-1)\n",
    "        )\n",
    "        output = self.output_linear(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b161850b-0195-48f8-838e-2380e92329ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioningModel(50,6,6,50).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "68d8b10c-6635-4923-a2ca-a0051544046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict = torch.load('model/model-adam.pt',map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dd1178de-5865-4d6f-b79a-25f0b47c6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_obj = torchvision.transforms.Resize((216,216))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "87678bd1-f472-44e4-b667-1cc1990f96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    img = Image.open(f\"{img_path}\").convert('RGB')\n",
    "    img = resize_obj(img)\n",
    "    inp =  preprocess(img).unsqueeze(0).to(device)\n",
    "    cap = torch.tensor(vocab(tokenizer(\"<start>\")),dtype=torch.int64).unsqueeze(0).to(device)\n",
    "    print(inp.shape)\n",
    "    print(cap.shape)\n",
    "    out = model(inp,cap)\n",
    "    predicted = torch.argmax(out,dim=-1)\n",
    "    #predicted = torch.max(out.data,1)[1]\n",
    "    text = [vocab.lookup_token(idx) for idx in predicted[0]]\n",
    "    text = set(text)\n",
    "    return text\n",
    "    #return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2e775dc3-4987-4f8e-8d5a-e204e064a9f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "text = list(predict('image.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c353f240-8e9a-4381-9c0a-ebb855548742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'over']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9e20a21d-83ef-4db2-8f76-47edd3452d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4802ed4d-f7ad-49b1-88f3-17ab8bf12620",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    if i == len(text) - 1:\n",
    "        \n",
    "        w += text[i]\n",
    "    else:\n",
    "        w += text[i] + ' and '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "882eec3f-0d5b-490c-9e11-1e1ffed7260d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blue and over'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6bbce9d2-9322-45bf-a9f6-0feaa9ab90c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out = gen(\"This image has characteristics of \" + w +\"could you tell something about it\",do_sample=False)[0]['generated_text'].split(' ')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "28d72a79-f9a0-474a-affc-c3a1a731c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ''\n",
    "for t in out:\n",
    "    pred += ' ' + t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cd73cf0b-3fba-4a3d-8ec4-5ccbe39ce25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This image has characteristics of blue and overcould you tell something about it?\\n\\nThe image is a composite of two images.'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c193d140-49ca-4398-a5f7-b7e46a7bac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your question do you think it is a scenery\n"
     ]
    }
   ],
   "source": [
    "user_ques = input('Your question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4ab94a55-a4fb-429d-b49a-799201e73ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key='AIzaSyCFJ3RwiHvLTy9QYMhraasRH1D3h7zZ2G0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "62f8e5e9-e3dc-43f6-8428-89b4835c45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=pred + user_ques,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fb06fa14-b3fb-41cb-bea6-0c36721e6ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, based on your description and the assumption that we\\'re dealing with a *composite* image with blue and overcast characteristics, and that you are asking about the possibility of it being a scenery image, here\\'s a breakdown of what we can infer:\\n\\n**What the Characteristics Suggest:**\\n\\n*   **Blue:** The presence of blue suggests several possibilities. It could be the sky, water (like a lake, sea, or river), or even blue-toned objects in the scene. If it\\'s prominent, it might indicate a clear day (although \"overcast\" contradicts this somewhat) or a specific mood created by the blue.\\n\\n*   **Overcast:** \"Overcast\" directly implies a cloudy sky. This means the lighting would likely be soft, diffused, and lack strong shadows. Colors would be muted compared to a sunny day.\\n\\n**Composite Nature and Scenery Possibility:**\\n\\n*   **Composite Image:** The fact that it\\'s a composite is crucial. It means elements from different images have been combined. This opens a wide range of possibilities. It could be:\\n    *   A realistic scenery that has been enhanced or altered with different images of sky and water.\\n    *   A completely unrealistic scenery where elements have been combined in a way that wouldn\\'t naturally occur.\\n    *   Not a scenery at all, but a blend of abstract elements or objects that were not originally together.\\n\\n*   **Scenery Probability:** Given the \"blue\" and \"overcast\" characteristics, it *is* likely the composite image *could* be trying to depict a scenery. The blue could be sky or water, and the overcast suggests an atmospheric condition. However, without seeing the image, it\\'s impossible to say for sure.\\n\\n**To determine with greater certainty if the image depicts a scenery, I would need to see it and consider the following:**\\n\\n*   **The overall composition:** Are there recognizable elements like trees, mountains, buildings, etc., arranged in a way that mimics a landscape?\\n*   **The relationship between the elements:** Do the blue and overcast elements blend convincingly with the other components of the image? Are the colors and lighting consistent?\\n*   **The level of realism:** Does the scene look natural, or is it clearly artificial or surreal?\\n*   **The intent of the artist:** What mood or message is the image trying to convey? This is hard to tell without more information about the artist or the context of the image.\\n\\nIn summary, the \"blue\" and \"overcast\" characteristics suggest a potential for scenery, but the fact that it\\'s a composite means it could be a very stylized, unrealistic, or even abstract interpretation of a landscape (or something else entirely).\\n'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
